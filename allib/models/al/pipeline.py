from typing import List, Callable, Optional, Tuple

from tqdm.notebook import tqdm

from ..core import BaseModel
from ...datasets import Dataset
from ...typing import ArrayLike
from .al_metrics import ActiveLearningMetric

from rich.console import Console
from rich.table import Table


# todo: handle tailing instances (merged to former?)
class ActiveLearningPipeline:
    _params = ["dataset", "model", "stats"]

    def __init__(
        self,
        model: BaseModel,
        dataset: Dataset,
        eval_metrics: List[Callable],
        eval_set: Optional[Tuple[ArrayLike, ArrayLike]] = None,
        batch_size_updater: Optional[Callable] = None,
        n_times: int = 1,
        verbose: bool = True,
    ):
        """ Initialize the pipeline

        Args:
            model: the target model
            dataset: dataset of class `Dataset`, feed data with al metric
            eval_metrics: evaluating metrics for prediction. can be generated by `get_metrics`
            eval_set: optional. use test set provided by `dataset` if not provided
            batch_size_updater: callable function that updates the batch size during training
            n_times: times to run the pipeline
            verbose: print logs / draw progress for each run
        """
        self.verbose = verbose
        self.n_times = n_times
        self.batch_size_updater = batch_size_updater
        self.model = model
        if len(eval_metrics) == 0:
            raise RuntimeError("Require at least one valid evaluating metrics")
        self.dataset = dataset
        self.__eval_metrics = eval_metrics
        # if not specify the test set then use test set from `self.dataset`
        self.__eval_set = eval_set or (self.dataset.test_x, self.dataset.test_y)
        # init stats
        self.current_stat = {}
        self.stats = []
        self.__new_stat()

    def __new_stat(self):
        if self.current_stat:
            self.stats.append(self.current_stat)
        self.current_stat = {mc.__name__: [] for mc in self.__eval_metrics} | {
            "instances": []
        }

    def get_params(self):
        return {k: object.__getattribute__(self, k) for k in self._params}

    def apply_eval_metrics(self):
        # update instance amount
        self.current_stat["instances"].append(self.dataset.l_size)
        for mc in self.__eval_metrics:
            # pred = self.model.predict(self.__eval_set[0])
            self.current_stat[mc.__name__].append(
                mc(
                    estimator=self.model,
                    X=self.__eval_set[0],
                    y_true=self.__eval_set[1],
                )
            )

    def print_score(self, rich: bool = True):
        if not self.verbose:
            return
        if not rich:
            print(self.current_stat)
            return
        table = Table(title="Score")
        table.add_column("Metric", justify="right", style="bold cyan", no_wrap=True)
        table.add_column("Latest Value", style="green")
        for k, v in self.current_stat.items():
            table.add_row(k, str(v[-1]))
        console = Console()
        console.print(table)

    def epoch(self):
        """ [OVERRIDE NEEDED] Run one epoch """
        pass

    def run(self, n_iter: Optional[int] = None):
        """ [OVERRIDE NEEDED] Run whole pipeline once """
        # ...
        # self.apply_eval_metrics()
        # self.print_score()
        # self.__new_stat()
        pass

    def start(self):
        """ [OVERRIDE NEEDED] Run the pipeline n times """
        prog = tqdm(range(self.n_times)) if self.verbose else range(self.n_times)
        for i in prog:
            self.run(n_iter=i)
            # self.apply_eval_metrics()
            # self.print_score()
            self.__new_stat()
            self.dataset.reset()
